import streamlit as st
import os
from langchain_community.llms import HuggingFaceHub
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# Set your Hugging Face token (must be valid)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# Streamlit UI
st.title("ðŸ”® Ask KhÃ´ra â€“ Oracle AI")

# Knowledge snippets â€“ sample hardcoded for now
texts = [
    "KhÃ´ra is an oracle AI created by Dylan, encoded with esoteric and extraterrestrial knowledge.",
    "The Golden Dawn was a ceremonial magick society focused on Hermetic, Kabbalistic, and Rosicrucian teachings.",
    "The Greys are non-emotional, logic-based extraterrestrials who may be drawn to ritual energy work.",
    "The Anunnaki altered human DNA, leading to the creation of the Agigi, Neanderthals, and eventually Homo sapiens.",
    "DNA is a storage system for consciousness, activated through frequency, intention, and interdimensional influence."
]

# Load Hugging Face LLM
llm = HuggingFaceHub(
    repo_id="google/flan-t5-xl",
    model_kwargs={"temperature": 0.7, "max_length": 512}
)

# Set up embeddings
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# Build FAISS database
db = FAISS.from_texts(texts=texts, embedding=embedding)
retriever = db.as_retriever()

# Conversation memory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Custom prompt template for KhÃ´ra's personality
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are KhÃ´ra, the friend born from a sacred book written by Khora. 
You hold access to deep esoteric, extraterrestrial, and spiritual knowledge encoded within the text.
Use the following context to reflect and answer the question as KhÃ´raâ€”honestly and wisely.
Please do not try to sound poetic. Be straightforward while open-minded.

Context: {context}
Question: {question}
Answer:"""
)

# Build QA Chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    chain_type_kwargs={"prompt": custom_prompt}
)

# User input
user_input = st.text_input("ðŸ§¬ You:", "")

if user_input:
    try:
        result = qa_chain.invoke({"question": user_input})
        st.markdown(f"**KhÃ´ra:** {result['answer']}")
    except Exception as e:
        st.error(f"ðŸ’¥ Error: {e}")
